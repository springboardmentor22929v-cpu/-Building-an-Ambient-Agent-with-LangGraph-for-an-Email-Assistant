{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a60e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-flash-latest\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c650e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
      "Expected Triage Output: respond\n",
      "Expected Tool Calls: ['write_email', 'done']\n",
      "Response Criteria: \n",
      "â€¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from email_dataset import email_inputs, expected_tool_calls, triage_outputs_list, response_criteria_list\n",
    "\n",
    "test_case_ix = 0\n",
    "\n",
    "print(\"Email Input:\", email_inputs[test_case_ix])\n",
    "print(\"Expected Triage Output:\", triage_outputs_list[test_case_ix])\n",
    "print(\"Expected Tool Calls:\", expected_tool_calls[test_case_ix])\n",
    "print(\"Response Criteria:\", response_criteria_list[test_case_ix])\n",
    "\n",
    "from utils import format_messages_string\n",
    "from email_assistant import email_assistant\n",
    "from utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\n",
    "    \n",
    "    This test confirms that all expected tools are called during email processing,\n",
    "    but does not check the order of tool invocations or the number of invocations\n",
    "    per tool. Additional checks for these aspects could be added if desired.\n",
    "    \"\"\"\n",
    "    # Run the email assistant\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "            \n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "            \n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "    \n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6be956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "from email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"E-mail Triage Evaluation\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, \n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbdd1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "    return {\"classification_decision\": response.update['classification_decision']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "568bc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeca6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'E-mail assistant workflow-fc4c7001' at:\n",
      "https://smith.langchain.com/o/e819c19c-8fe1-4341-b37f-842fdba883d5/datasets/26710b80-4a33-4f9b-8056-4afcacbe450e/compare?selectedSessions=dd18fbba-47fb-4f4e-9287-3664c9046331\n",
      "\n",
      "\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸš« Classification: IGNORE - This email can be safely ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error calling model 'gemini-flash-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 428.998527ms.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 3040, in _generate\n",
      "    response: GenerateContentResponse = self.client.models.generate_content(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 5215, in generate_content\n",
      "    response = self._generate_content(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 3997, in _generate_content\n",
      "    response = self._api_client.request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1375, in request\n",
      "    response = self._request(http_request, http_options, stream=False)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1209, in _request\n",
      "    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1188, in _request_once\n",
      "    errors.APIError.raise_for_response(response)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\errors.py\", line 121, in raise_for_response\n",
      "    cls.raise_error(response.status_code, response_json, response)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\errors.py\", line 146, in raise_error\n",
      "    raise ClientError(status_code, response_json, response)\n",
      "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 428.998527ms.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1898, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\1330990688.py\", line 3, in target_email_assistant\n",
      "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_read.py\", line 233, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\notebooks1\\email_assistant.py\", line 121, in triage_router\n",
      "    result = llm_router.invoke(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3149, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5557, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 2535, in invoke\n",
      "    return super().invoke(input, config, stop=stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 3044, in _generate\n",
      "    _handle_client_error(e, request)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 145, in _handle_client_error\n",
      "    raise ChatGoogleGenerativeAIError(msg) from e\n",
      "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-flash-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 428.998527ms.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}\n",
      "Error running evaluator <DynamicRunEvaluator classification_evaluator> on run 019c1ded-b615-75e0-9b5b-9dbce2c6f5c2: KeyError('classification_decision')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1596, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 332, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 758, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\3876906441.py\", line 3, in classification_evaluator\n",
      "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()\n",
      "           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'classification_decision'\n",
      "Error running target function: Error calling model 'gemini-flash-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 59.846655806s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '59s'}]}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 3040, in _generate\n",
      "    response: GenerateContentResponse = self.client.models.generate_content(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 5215, in generate_content\n",
      "    response = self._generate_content(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 3997, in _generate_content\n",
      "    response = self._api_client.request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1375, in request\n",
      "    response = self._request(http_request, http_options, stream=False)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1209, in _request\n",
      "    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1188, in _request_once\n",
      "    errors.APIError.raise_for_response(response)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\errors.py\", line 121, in raise_for_response\n",
      "    cls.raise_error(response.status_code, response_json, response)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\errors.py\", line 146, in raise_error\n",
      "    raise ClientError(status_code, response_json, response)\n",
      "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 59.846655806s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '59s'}]}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1898, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\1330990688.py\", line 3, in target_email_assistant\n",
      "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_read.py\", line 233, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\notebooks1\\email_assistant.py\", line 121, in triage_router\n",
      "    result = llm_router.invoke(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3149, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5557, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 2535, in invoke\n",
      "    return super().invoke(input, config, stop=stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 3044, in _generate\n",
      "    _handle_client_error(e, request)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 145, in _handle_client_error\n",
      "    raise ChatGoogleGenerativeAIError(msg) from e\n",
      "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-flash-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 59.846655806s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '59s'}]}}\n",
      "Error running evaluator <DynamicRunEvaluator classification_evaluator> on run 019c1ded-bd26-7d40-aa5e-ed72288bceaf: KeyError('classification_decision')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1596, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 332, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 758, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\3876906441.py\", line 3, in classification_evaluator\n",
      "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()\n",
      "           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'classification_decision'\n",
      "Error running target function: Server disconnected without sending a response.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 106, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 177, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 231, in _receive_event\n",
      "    raise RemoteProtocolError(msg)\n",
      "httpcore.RemoteProtocolError: Server disconnected without sending a response.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1898, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\1330990688.py\", line 3, in target_email_assistant\n",
      "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_read.py\", line 233, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\notebooks1\\email_assistant.py\", line 121, in triage_router\n",
      "    result = llm_router.invoke(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3149, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5557, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 2535, in invoke\n",
      "    return super().invoke(input, config, stop=stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 3040, in _generate\n",
      "    response: GenerateContentResponse = self.client.models.generate_content(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 5215, in generate_content\n",
      "    response = self._generate_content(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 3997, in _generate_content\n",
      "    response = self._api_client.request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1375, in request\n",
      "    response = self._request(http_request, http_options, stream=False)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1209, in _request\n",
      "    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 400, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1181, in _request_once\n",
      "    response = self._httpx_client.request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.RemoteProtocolError: Server disconnected without sending a response.\n",
      "Error running evaluator <DynamicRunEvaluator classification_evaluator> on run 019c1dee-4202-7fe0-a84a-49be97bc0400: KeyError('classification_decision')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1596, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 332, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 758, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\3876906441.py\", line 3, in classification_evaluator\n",
      "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()\n",
      "           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'classification_decision'\n",
      "Error running target function: Server disconnected without sending a response.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 106, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 177, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 231, in _receive_event\n",
      "    raise RemoteProtocolError(msg)\n",
      "httpcore.RemoteProtocolError: Server disconnected without sending a response.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1898, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\1330990688.py\", line 3, in target_email_assistant\n",
      "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_read.py\", line 233, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\notebooks1\\email_assistant.py\", line 121, in triage_router\n",
      "    result = llm_router.invoke(\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3149, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5557, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 2535, in invoke\n",
      "    return super().invoke(input, config, stop=stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 3040, in _generate\n",
      "    response: GenerateContentResponse = self.client.models.generate_content(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 5215, in generate_content\n",
      "    response = self._generate_content(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\models.py\", line 3997, in _generate_content\n",
      "    response = self._api_client.request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1375, in request\n",
      "    response = self._request(http_request, http_options, stream=False)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1209, in _request\n",
      "    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 400, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py\", line 1181, in _request_once\n",
      "    response = self._httpx_client.request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.RemoteProtocolError: Server disconnected without sending a response.\n",
      "Error running evaluator <DynamicRunEvaluator classification_evaluator> on run 019c1dee-56b2-70d2-a88b-7e9c738f28b1: KeyError('classification_decision')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1596, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 332, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 716, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\B.NANDINI\\Ambient_agent_1\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 758, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\B.NANDINI\\AppData\\Local\\Temp\\ipykernel_11928\\3876906441.py\", line 3, in classification_evaluator\n",
      "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()\n",
      "           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'classification_decision'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n"
     ]
    }
   ],
   "source": [
    "run_expt = True\n",
    "if run_expt:\n",
    "    experiment_results_workflow = client.evaluate(\n",
    "        # Run agent \n",
    "        target_email_assistant,\n",
    "        # Dataset name   \n",
    "        data=dataset_name,\n",
    "        # Evaluator\n",
    "        evaluators=[classification_evaluator],\n",
    "        # Name of the experiment\n",
    "        experiment_prefix=\"E-mail assistant workflow\", \n",
    "        # Number of concurrent evaluations\n",
    "        max_concurrency=2, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da97c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Score the response against specific criteria.\"\"\"\n",
    "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
    "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
    "    \n",
    "# Create a global LLM for evaluation to avoid recreating it for each test\n",
    "criteria_eval_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-flash-latest\",\n",
    "    temperature=0\n",
    ")\n",
    "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(CriteriaGrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_input = email_inputs[0]\n",
    "print(\"Email Input:\", email_input)\n",
    "success_criteria = response_criteria_list[0]\n",
    "print(\"Success Criteria:\", success_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = email_assistant.invoke({\"email_input\": email_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3274cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
    "\n",
    "all_messages_str = format_messages_string(response['messages'])\n",
    "eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {success_criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "\n",
    "eval_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
